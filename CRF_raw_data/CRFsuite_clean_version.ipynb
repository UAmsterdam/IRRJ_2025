{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a21c96d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import sklearn_crfsuite\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from datasets import Dataset\n",
    "import pickle\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df0f13fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>index</th>\n",
       "      <th>sent_id</th>\n",
       "      <th>topic_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EXECUTION VERSION</td>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "      <td>150705_0</td>\n",
       "      <td>1252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>THE BANK OF NOVA SCOTIA</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>150705_1</td>\n",
       "      <td>1252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>as Sole Bookrunner , Lead Arranger and Adminis...</td>\n",
       "      <td>B</td>\n",
       "      <td>2</td>\n",
       "      <td>150705_2</td>\n",
       "      <td>1252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>- and-</td>\n",
       "      <td>B</td>\n",
       "      <td>3</td>\n",
       "      <td>150705_3</td>\n",
       "      <td>1252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BMO CAPITAL MARKETS</td>\n",
       "      <td>B</td>\n",
       "      <td>4</td>\n",
       "      <td>150705_4</td>\n",
       "      <td>1252</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence label  index   sent_id  \\\n",
       "0                                  EXECUTION VERSION     B      0  150705_0   \n",
       "1                            THE BANK OF NOVA SCOTIA     B      1  150705_1   \n",
       "2  as Sole Bookrunner , Lead Arranger and Adminis...     B      2  150705_2   \n",
       "3                                             - and-     B      3  150705_3   \n",
       "4                                BMO CAPITAL MARKETS     B      4  150705_4   \n",
       "\n",
       "   topic_id  \n",
       "0      1252  \n",
       "1      1252  \n",
       "2      1252  \n",
       "3      1252  \n",
       "4      1252  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('due_dilligence_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae315b70",
   "metadata": {},
   "source": [
    "### Load the custom punkt tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ff7ba4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "with open('custom_punkt_tokenizer.pkl', 'rb') as f:\n",
    "    loaded_tokenizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468bc6ef",
   "metadata": {},
   "source": [
    "### Feature creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62f727d",
   "metadata": {},
   "source": [
    "## Training CRF model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6c0cf5",
   "metadata": {},
   "source": [
    "### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a65bfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## train function\n",
    "from itertools import chain\n",
    "\n",
    "def train_crf_model(train_data, test_data, model_save_dir, topic_id, fold):\n",
    "    ''' This function is training and saving CRF model for each fold.'''\n",
    "    \n",
    "    train_sentences = train_data['sentence']\n",
    "    train_labels = train_data['label']\n",
    "    \n",
    "    # Extract features and labels for each training\n",
    "    train_extracted = [process_text_and_extract_features(sentence, label) for sentence, label in zip(train_sentences, train_labels)]\n",
    "    \n",
    "    \n",
    "    X_train = [features for features, _ in train_extracted]\n",
    "    y_train = [labels for _, labels in train_extracted]\n",
    "\n",
    "    print(train_sentences[:2])\n",
    "    \n",
    "    print(\"First sentence features in training data:\", X_train[:2])\n",
    "    print(\"First sentence labels in training data:\", y_train[:2])\n",
    "\n",
    "#     esd\n",
    "    print(f\"Training model for topic {topic_id} for fold {fold}\")\n",
    "\n",
    "    crf = sklearn_crfsuite.CRF(algorithm='pa', c=0.1, pa_type=2, max_iterations=100,\n",
    "                               all_possible_transitions=True, verbose=True)\n",
    "    crf.fit(X_train, y_train)\n",
    "    \n",
    "    # Save the trained CRF model for the topic\n",
    "    fold_model_dir = os.path.join(model_save_dir, topic_id)\n",
    "    \n",
    "    # Create the directory if it does not exist\n",
    "    os.makedirs(fold_model_dir, exist_ok=True)\n",
    "\n",
    "    # Define the full path for the model file\n",
    "    fold_model_path = os.path.join(fold_model_dir, f\"{topic_id}_crf_model_{fold}.pkl\")\n",
    "\n",
    "    # Save the model\n",
    "    with open(fold_model_path, \"wb\") as model_file:\n",
    "        pickle.dump(crf, model_file)\n",
    "\n",
    "    print(f\"CRF model saved for topic {topic_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f140261",
   "metadata": {},
   "source": [
    "### Functions to load the data using predefined folds for 5 folds cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80a3d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = 'core/qrels/'\n",
    "\n",
    "def map_doc(row):\n",
    "    return row['sent_id'].split('_')[0]\n",
    "\n",
    "def read_split(file_):\n",
    "    return [el for el in open(file_).read().split('\\n') if el != '']\n",
    "\n",
    "def map_doc(row):\n",
    "    return row['sent_id'].split('_')[0]\n",
    "\n",
    "def stratify(df):\n",
    "    g = df.groupby('label')\n",
    "    return df, g.apply(lambda x: x.sample(g.size().min()).reset_index(drop=True))\n",
    "\n",
    "# List of folder names you want to process\n",
    "# folders_to_process = ['1272', '1474','1238', '1275', '1239', '1520', '1509', '1240', '1308', '1319', '1439', \n",
    "#                  '1267', '1242', '1462', '1265', '1444', '1312', '1244', '1243', '1468', '1309', '1524', \n",
    "#                  '1247', '1440', '1251', '1249', '1248', '1262', '1250', '1252', '1245', '1512', '1498', \n",
    "#                  '1601', '1443', '1086', '1551', '1253', '1320', '1304', '1469', '1611', '1300', '1489', \n",
    "#                  '1500', '1261', '1318', '1460', '1475', '1321']\n",
    "\n",
    "folders_to_process = [ '1250']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905bd48d",
   "metadata": {},
   "source": [
    "### Data prepration and model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffa368e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for fold: 0\n",
      "label\n",
      "1    25\n",
      "B    25\n",
      "Name: count, dtype: int64 label\n",
      "B    179377\n",
      "1        25\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "1    106\n",
      "B    106\n",
      "Name: count, dtype: int64 label\n",
      "B    752019\n",
      "1       106\n",
      "Name: count, dtype: int64\n",
      "32413173          EXECUTION VERSION\n",
      "32413174    THE BANK OF NOVA SCOTIA\n",
      "Name: sentence, dtype: object\n",
      "First sentence features in training data: [[{'token': 'EXECUTION', 'lower': 'execution', 'is_first': True, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'E', 'prefix-2': 'EX', 'prefix-3': 'EXE', 'suffix-1': 'N', 'suffix-2': 'ON', 'suffix-3': 'ION', 'prev_token': '', 'next_token': 'VERSION', 'is_numeric': False, 'unigram': 'EXECUTION', 'bigram': 'EXECUTION VERSION', 'trigram': ''}, {'token': 'VERSION', 'lower': 'version', 'is_first': False, 'is_last': True, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'V', 'prefix-2': 'VE', 'prefix-3': 'VER', 'suffix-1': 'N', 'suffix-2': 'ON', 'suffix-3': 'ION', 'prev_token': 'EXECUTION', 'next_token': '', 'is_numeric': False, 'unigram': 'VERSION', 'bigram': '', 'trigram': ''}], [{'token': 'THE', 'lower': 'the', 'is_first': True, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'T', 'prefix-2': 'TH', 'prefix-3': 'THE', 'suffix-1': 'E', 'suffix-2': 'HE', 'suffix-3': 'THE', 'prev_token': '', 'next_token': 'BANK', 'is_numeric': False, 'unigram': 'THE', 'bigram': 'THE BANK', 'trigram': 'THE BANK OF'}, {'token': 'BANK', 'lower': 'bank', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'B', 'prefix-2': 'BA', 'prefix-3': 'BAN', 'suffix-1': 'K', 'suffix-2': 'NK', 'suffix-3': 'ANK', 'prev_token': 'THE', 'next_token': 'OF', 'is_numeric': False, 'unigram': 'BANK', 'bigram': 'BANK OF', 'trigram': 'BANK OF NOVA'}, {'token': 'OF', 'lower': 'of', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'O', 'prefix-2': 'OF', 'prefix-3': 'OF', 'suffix-1': 'F', 'suffix-2': 'OF', 'suffix-3': 'OF', 'prev_token': 'BANK', 'next_token': 'NOVA', 'is_numeric': False, 'unigram': 'OF', 'bigram': 'OF NOVA', 'trigram': 'OF NOVA SCOTIA'}, {'token': 'NOVA', 'lower': 'nova', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'N', 'prefix-2': 'NO', 'prefix-3': 'NOV', 'suffix-1': 'A', 'suffix-2': 'VA', 'suffix-3': 'OVA', 'prev_token': 'OF', 'next_token': 'SCOTIA', 'is_numeric': False, 'unigram': 'NOVA', 'bigram': 'NOVA SCOTIA', 'trigram': ''}, {'token': 'SCOTIA', 'lower': 'scotia', 'is_first': False, 'is_last': True, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'S', 'prefix-2': 'SC', 'prefix-3': 'SCO', 'suffix-1': 'A', 'suffix-2': 'IA', 'suffix-3': 'TIA', 'prev_token': 'NOVA', 'next_token': '', 'is_numeric': False, 'unigram': 'SCOTIA', 'bigram': '', 'trigram': ''}]]\n",
      "First sentence labels in training data: [['B', 'B'], ['B', 'B', 'B', 'B', 'B']]\n",
      "Training model for topic 1250 for fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading training data to CRFsuite: 100%|█| 752125/752125 [03:59<00:00, 3140.20it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature generation\n",
      "type: CRF1d\n",
      "feature.minfreq: 0.000000\n",
      "feature.possible_states: 0\n",
      "feature.possible_transitions: 1\n",
      "0....1....2....3....4....5....6....7....8....9....10\n",
      "Number of features: 2807308\n",
      "Seconds required: 62.713\n",
      "\n",
      "Passive Aggressive\n",
      "type: 2\n",
      "c: 0.100000\n",
      "error_sensitive: 1\n",
      "averaging: 1\n",
      "max_iterations: 100\n",
      "epsilon: 0.000000\n",
      "\n",
      "Iter 1   time=6.46  loss=2948.28  feature_norm=1.20\n",
      "Iter 2   time=6.16  loss=2316.47  feature_norm=1.67\n",
      "Iter 3   time=6.22  loss=2015.79  feature_norm=1.99\n",
      "Iter 4   time=6.73  loss=1449.58  feature_norm=2.20\n",
      "Iter 5   time=6.47  loss=1181.80  feature_norm=2.37\n",
      "Iter 6   time=6.36  loss=1072.01  feature_norm=2.49\n",
      "Iter 7   time=6.25  loss=913.89   feature_norm=2.59\n",
      "Iter 8   time=6.10  loss=732.78   feature_norm=2.68\n",
      "Iter 9   time=6.33  loss=747.02   feature_norm=2.75\n",
      "Iter 10  time=6.11  loss=594.49   feature_norm=2.79\n",
      "Iter 11  time=6.14  loss=582.27   feature_norm=2.84\n",
      "Iter 12  time=6.11  loss=464.35   feature_norm=2.88\n",
      "Iter 13  time=6.34  loss=609.99   feature_norm=2.94\n",
      "Iter 14  time=6.42  loss=365.12   feature_norm=2.96\n",
      "Iter 15  time=6.06  loss=356.07   feature_norm=3.00\n",
      "Iter 16  time=6.09  loss=446.34   feature_norm=3.04\n",
      "Iter 17  time=6.14  loss=281.41   feature_norm=3.06\n",
      "Iter 18  time=6.18  loss=455.98   feature_norm=3.11\n",
      "Iter 19  time=6.34  loss=275.01   feature_norm=3.13\n",
      "Iter 20  time=6.41  loss=453.10   feature_norm=3.17\n",
      "Iter 21  time=6.25  loss=230.99   feature_norm=3.20\n",
      "Iter 22  time=6.23  loss=379.96   feature_norm=3.23\n",
      "Iter 23  time=6.39  loss=216.03   feature_norm=3.25\n",
      "Iter 24  time=6.24  loss=298.67   feature_norm=3.28\n",
      "Iter 25  time=6.24  loss=242.82   feature_norm=3.30\n",
      "Iter 26  time=6.17  loss=395.01   feature_norm=3.33\n",
      "Iter 27  time=6.29  loss=286.70   feature_norm=3.35\n",
      "Iter 28  time=6.19  loss=301.05   feature_norm=3.37\n",
      "Iter 29  time=6.28  loss=291.76   feature_norm=3.40\n",
      "Iter 30  time=6.10  loss=268.44   feature_norm=3.42\n",
      "Iter 31  time=6.12  loss=157.32   feature_norm=3.43\n",
      "Iter 32  time=6.45  loss=118.21   feature_norm=3.45\n",
      "Iter 33  time=6.34  loss=301.32   feature_norm=3.47\n",
      "Iter 34  time=6.16  loss=181.93   feature_norm=3.48\n",
      "Iter 35  time=6.15  loss=232.37   feature_norm=3.50\n",
      "Iter 36  time=6.16  loss=240.59   feature_norm=3.51\n",
      "Iter 37  time=6.39  loss=82.27    feature_norm=3.52\n",
      "Iter 38  time=6.20  loss=128.50   feature_norm=3.53\n",
      "Iter 39  time=6.13  loss=296.78   feature_norm=3.56\n",
      "Iter 40  time=6.41  loss=238.46   feature_norm=3.57\n",
      "Iter 41  time=6.28  loss=225.22   feature_norm=3.59\n",
      "Iter 42  time=6.26  loss=42.86    feature_norm=3.59\n",
      "Iter 43  time=6.20  loss=266.18   feature_norm=3.61\n",
      "Iter 44  time=6.15  loss=196.25   feature_norm=3.62\n",
      "Iter 45  time=6.29  loss=213.85   feature_norm=3.65\n",
      "Iter 46  time=6.34  loss=266.77   feature_norm=3.67\n",
      "Iter 47  time=6.15  loss=255.74   feature_norm=3.69\n",
      "Iter 48  time=6.22  loss=248.90   feature_norm=3.71\n",
      "Iter 49  time=6.18  loss=316.02   feature_norm=3.72\n",
      "Iter 50  time=6.58  loss=239.97   feature_norm=3.74\n",
      "Iter 51  time=6.50  loss=214.02   feature_norm=3.75\n",
      "Iter 52  time=6.18  loss=210.25   feature_norm=3.76\n",
      "Iter 53  time=6.13  loss=143.65   feature_norm=3.77\n",
      "Iter 54  time=6.15  loss=256.13   feature_norm=3.79\n",
      "Iter 55  time=6.16  loss=177.66   feature_norm=3.80\n",
      "Iter 56  time=6.25  loss=109.55   feature_norm=3.80\n",
      "Iter 57  time=6.21  loss=108.82   feature_norm=3.81\n",
      "Iter 58  time=6.31  loss=168.19   feature_norm=3.82\n",
      "Iter 59  time=6.08  loss=150.33   feature_norm=3.83\n",
      "Iter 60  time=6.38  loss=71.29    feature_norm=3.83\n",
      "Iter 61  time=6.22  loss=59.79    feature_norm=3.83\n",
      "Iter 62  time=6.23  loss=51.07    feature_norm=3.83\n",
      "Iter 63  time=6.27  loss=48.08    feature_norm=3.84\n",
      "Iter 64  time=6.18  loss=137.45   feature_norm=3.85\n",
      "Iter 65  time=6.30  loss=141.33   feature_norm=3.86\n",
      "Iter 66  time=6.08  loss=50.45    feature_norm=3.86\n",
      "Iter 67  time=6.37  loss=78.66    feature_norm=3.87\n",
      "Iter 68  time=6.14  loss=145.81   feature_norm=3.89\n",
      "Iter 69  time=6.50  loss=82.12    feature_norm=3.90\n",
      "Iter 70  time=6.09  loss=143.60   feature_norm=3.91\n",
      "Iter 71  time=6.08  loss=78.82    feature_norm=3.92\n",
      "Iter 72  time=6.32  loss=185.53   feature_norm=3.93\n",
      "Iter 73  time=6.65  loss=121.37   feature_norm=3.93\n",
      "Iter 74  time=6.61  loss=45.26    feature_norm=3.93\n",
      "Iter 75  time=6.41  loss=23.33    feature_norm=3.94\n",
      "Iter 76  time=6.50  loss=122.09   feature_norm=3.94\n",
      "Iter 77  time=6.22  loss=44.35    feature_norm=3.94\n",
      "Iter 78  time=6.59  loss=126.08   feature_norm=3.95\n",
      "Iter 79  time=6.23  loss=135.32   feature_norm=3.96\n",
      "Iter 80  time=6.30  loss=6.10     feature_norm=3.96\n",
      "Iter 81  time=6.28  loss=51.13    feature_norm=3.96\n",
      "Iter 82  time=6.26  loss=8.76     feature_norm=3.96\n",
      "Iter 83  time=6.21  loss=8.82     feature_norm=3.96\n",
      "Iter 84  time=6.26  loss=153.47   feature_norm=3.97\n",
      "Iter 85  time=6.40  loss=95.27    feature_norm=3.97\n",
      "Iter 86  time=6.14  loss=98.39    feature_norm=3.99\n",
      "Iter 87  time=6.58  loss=35.18    feature_norm=3.99\n",
      "Iter 88  time=6.25  loss=54.20    feature_norm=4.00\n",
      "Iter 89  time=6.17  loss=120.16   feature_norm=4.00\n",
      "Iter 90  time=6.12  loss=195.18   feature_norm=4.02\n",
      "Iter 91  time=6.21  loss=184.35   feature_norm=4.02\n",
      "Iter 92  time=6.25  loss=55.57    feature_norm=4.03\n",
      "Iter 93  time=6.39  loss=37.69    feature_norm=4.03\n",
      "Iter 94  time=6.22  loss=8.76     feature_norm=4.03\n",
      "Iter 95  time=6.25  loss=30.02    feature_norm=4.03\n",
      "Iter 96  time=6.41  loss=33.03    feature_norm=4.03\n",
      "Iter 97  time=6.38  loss=79.45    feature_norm=4.03\n",
      "Iter 98  time=6.20  loss=37.07    feature_norm=4.03\n",
      "Iter 99  time=6.25  loss=44.03    feature_norm=4.04\n",
      "Iter 100 time=6.19  loss=48.91    feature_norm=4.04\n",
      "Total seconds required for training: 627.067\n",
      "\n",
      "Storing the model\n",
      "Number of active features: 72595 (2807308)\n",
      "Number of active attributes: 63792 (2798504)\n",
      "Number of active labels: 2 (2)\n",
      "Writing labels\n",
      "Writing attributes\n",
      "Writing feature references for transitions\n",
      "Writing feature references for attributes\n",
      "Seconds required: 0.189\n",
      "\n",
      "CRF model saved for topic 1250\n",
      "['B', 'B', 'B']\n",
      "['B', 'B', 'B']\n",
      "Precision: 0.9869024224054338, Recall: 0.8818022230065522, F1: 0.9313962871619642\n",
      "Topic 1250 - Avg Precision: 0.9869, Avg Recall: 0.8818, Avg F1-Score: 0.9314\n",
      "for fold: 1\n",
      "label\n",
      "1    26\n",
      "B    26\n",
      "Name: count, dtype: int64 label\n",
      "B    185433\n",
      "1        26\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "1    105\n",
      "B    105\n",
      "Name: count, dtype: int64 label\n",
      "B    745963\n",
      "1       105\n",
      "Name: count, dtype: int64\n",
      "32413173          EXECUTION VERSION\n",
      "32413174    THE BANK OF NOVA SCOTIA\n",
      "Name: sentence, dtype: object\n",
      "First sentence features in training data: [[{'token': 'EXECUTION', 'lower': 'execution', 'is_first': True, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'E', 'prefix-2': 'EX', 'prefix-3': 'EXE', 'suffix-1': 'N', 'suffix-2': 'ON', 'suffix-3': 'ION', 'prev_token': '', 'next_token': 'VERSION', 'is_numeric': False, 'unigram': 'EXECUTION', 'bigram': 'EXECUTION VERSION', 'trigram': ''}, {'token': 'VERSION', 'lower': 'version', 'is_first': False, 'is_last': True, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'V', 'prefix-2': 'VE', 'prefix-3': 'VER', 'suffix-1': 'N', 'suffix-2': 'ON', 'suffix-3': 'ION', 'prev_token': 'EXECUTION', 'next_token': '', 'is_numeric': False, 'unigram': 'VERSION', 'bigram': '', 'trigram': ''}], [{'token': 'THE', 'lower': 'the', 'is_first': True, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'T', 'prefix-2': 'TH', 'prefix-3': 'THE', 'suffix-1': 'E', 'suffix-2': 'HE', 'suffix-3': 'THE', 'prev_token': '', 'next_token': 'BANK', 'is_numeric': False, 'unigram': 'THE', 'bigram': 'THE BANK', 'trigram': 'THE BANK OF'}, {'token': 'BANK', 'lower': 'bank', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'B', 'prefix-2': 'BA', 'prefix-3': 'BAN', 'suffix-1': 'K', 'suffix-2': 'NK', 'suffix-3': 'ANK', 'prev_token': 'THE', 'next_token': 'OF', 'is_numeric': False, 'unigram': 'BANK', 'bigram': 'BANK OF', 'trigram': 'BANK OF NOVA'}, {'token': 'OF', 'lower': 'of', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'O', 'prefix-2': 'OF', 'prefix-3': 'OF', 'suffix-1': 'F', 'suffix-2': 'OF', 'suffix-3': 'OF', 'prev_token': 'BANK', 'next_token': 'NOVA', 'is_numeric': False, 'unigram': 'OF', 'bigram': 'OF NOVA', 'trigram': 'OF NOVA SCOTIA'}, {'token': 'NOVA', 'lower': 'nova', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'N', 'prefix-2': 'NO', 'prefix-3': 'NOV', 'suffix-1': 'A', 'suffix-2': 'VA', 'suffix-3': 'OVA', 'prev_token': 'OF', 'next_token': 'SCOTIA', 'is_numeric': False, 'unigram': 'NOVA', 'bigram': 'NOVA SCOTIA', 'trigram': ''}, {'token': 'SCOTIA', 'lower': 'scotia', 'is_first': False, 'is_last': True, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'S', 'prefix-2': 'SC', 'prefix-3': 'SCO', 'suffix-1': 'A', 'suffix-2': 'IA', 'suffix-3': 'TIA', 'prev_token': 'NOVA', 'next_token': '', 'is_numeric': False, 'unigram': 'SCOTIA', 'bigram': '', 'trigram': ''}]]\n",
      "First sentence labels in training data: [['B', 'B'], ['B', 'B', 'B', 'B', 'B']]\n",
      "Training model for topic 1250 for fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading training data to CRFsuite: 100%|█| 746068/746068 [04:18<00:00, 2881.39it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature generation\n",
      "type: CRF1d\n",
      "feature.minfreq: 0.000000\n",
      "feature.possible_states: 0\n",
      "feature.possible_transitions: 1\n",
      "0....1....2....3....4....5....6....7....8....9....10\n",
      "Number of features: 2859236\n",
      "Seconds required: 60.677\n",
      "\n",
      "Passive Aggressive\n",
      "type: 2\n",
      "c: 0.100000\n",
      "error_sensitive: 1\n",
      "averaging: 1\n",
      "max_iterations: 100\n",
      "epsilon: 0.000000\n",
      "\n",
      "Iter 1   time=7.26  loss=3164.31  feature_norm=1.20\n",
      "Iter 2   time=6.66  loss=2173.70  feature_norm=1.66\n",
      "Iter 3   time=6.57  loss=1767.42  feature_norm=1.95\n",
      "Iter 4   time=6.50  loss=1489.02  feature_norm=2.17\n",
      "Iter 5   time=6.62  loss=1299.38  feature_norm=2.35\n",
      "Iter 6   time=6.52  loss=947.41   feature_norm=2.48\n",
      "Iter 7   time=6.54  loss=1071.63  feature_norm=2.59\n",
      "Iter 8   time=6.46  loss=719.61   feature_norm=2.68\n",
      "Iter 9   time=6.84  loss=934.32   feature_norm=2.78\n",
      "Iter 10  time=6.78  loss=789.49   feature_norm=2.88\n",
      "Iter 11  time=6.68  loss=620.79   feature_norm=2.95\n",
      "Iter 12  time=6.56  loss=590.77   feature_norm=2.99\n",
      "Iter 13  time=6.54  loss=454.22   feature_norm=3.04\n",
      "Iter 14  time=6.57  loss=557.06   feature_norm=3.11\n",
      "Iter 15  time=6.57  loss=523.78   feature_norm=3.16\n",
      "Iter 16  time=6.63  loss=499.64   feature_norm=3.21\n",
      "Iter 17  time=6.57  loss=519.40   feature_norm=3.26\n",
      "Iter 18  time=7.05  loss=458.67   feature_norm=3.31\n",
      "Iter 19  time=6.65  loss=309.17   feature_norm=3.34\n",
      "Iter 20  time=6.64  loss=463.02   feature_norm=3.37\n",
      "Iter 21  time=6.43  loss=374.52   feature_norm=3.41\n",
      "Iter 22  time=6.59  loss=265.53   feature_norm=3.44\n",
      "Iter 23  time=6.46  loss=242.85   feature_norm=3.45\n",
      "Iter 24  time=6.58  loss=349.43   feature_norm=3.48\n",
      "Iter 25  time=6.61  loss=242.29   feature_norm=3.50\n",
      "Iter 26  time=6.58  loss=272.34   feature_norm=3.52\n",
      "Iter 27  time=6.84  loss=203.30   feature_norm=3.54\n",
      "Iter 28  time=6.50  loss=316.56   feature_norm=3.58\n",
      "Iter 29  time=6.49  loss=292.40   feature_norm=3.60\n",
      "Iter 30  time=6.53  loss=243.21   feature_norm=3.61\n",
      "Iter 31  time=6.65  loss=196.69   feature_norm=3.63\n",
      "Iter 32  time=6.76  loss=267.65   feature_norm=3.65\n",
      "Iter 33  time=6.53  loss=214.56   feature_norm=3.66\n",
      "Iter 34  time=6.57  loss=268.87   feature_norm=3.68\n",
      "Iter 35  time=6.76  loss=211.30   feature_norm=3.69\n",
      "Iter 36  time=6.63  loss=148.40   feature_norm=3.70\n",
      "Iter 37  time=6.61  loss=268.95   feature_norm=3.72\n",
      "Iter 38  time=6.49  loss=217.42   feature_norm=3.73\n",
      "Iter 39  time=6.49  loss=265.08   feature_norm=3.74\n",
      "Iter 40  time=6.50  loss=210.31   feature_norm=3.75\n",
      "Iter 41  time=6.45  loss=265.37   feature_norm=3.77\n",
      "Iter 42  time=6.50  loss=77.62    feature_norm=3.77\n",
      "Iter 43  time=6.54  loss=102.26   feature_norm=3.78\n",
      "Iter 44  time=7.01  loss=211.12   feature_norm=3.80\n",
      "Iter 45  time=6.56  loss=193.54   feature_norm=3.81\n",
      "Iter 46  time=6.55  loss=103.02   feature_norm=3.81\n",
      "Iter 47  time=6.67  loss=152.75   feature_norm=3.83\n",
      "Iter 48  time=6.58  loss=101.33   feature_norm=3.84\n",
      "Iter 49  time=6.48  loss=157.33   feature_norm=3.85\n",
      "Iter 50  time=6.47  loss=138.26   feature_norm=3.87\n",
      "Iter 51  time=6.56  loss=112.21   feature_norm=3.87\n",
      "Iter 52  time=6.63  loss=81.17    feature_norm=3.88\n",
      "Iter 53  time=6.73  loss=101.34   feature_norm=3.88\n",
      "Iter 54  time=6.75  loss=14.41    feature_norm=3.89\n",
      "Iter 55  time=6.57  loss=185.94   feature_norm=3.89\n",
      "Iter 56  time=6.56  loss=245.48   feature_norm=3.91\n",
      "Iter 57  time=6.59  loss=90.20    feature_norm=3.92\n",
      "Iter 58  time=6.71  loss=47.73    feature_norm=3.92\n",
      "Iter 59  time=6.44  loss=171.63   feature_norm=3.94\n",
      "Iter 60  time=6.55  loss=226.52   feature_norm=3.95\n",
      "Iter 61  time=6.73  loss=139.09   feature_norm=3.96\n",
      "Iter 62  time=6.78  loss=123.99   feature_norm=3.96\n",
      "Iter 63  time=6.81  loss=130.74   feature_norm=3.98\n",
      "Iter 64  time=6.53  loss=138.67   feature_norm=3.99\n",
      "Iter 65  time=6.47  loss=119.86   feature_norm=3.99\n",
      "Iter 66  time=6.62  loss=87.55    feature_norm=4.00\n",
      "Iter 67  time=6.56  loss=163.78   feature_norm=4.01\n",
      "Iter 68  time=6.48  loss=197.96   feature_norm=4.03\n",
      "Iter 69  time=6.51  loss=46.15    feature_norm=4.03\n",
      "Iter 70  time=6.62  loss=106.29   feature_norm=4.03\n",
      "Iter 71  time=6.65  loss=135.76   feature_norm=4.05\n",
      "Iter 72  time=6.63  loss=248.86   feature_norm=4.07\n",
      "Iter 73  time=6.60  loss=219.49   feature_norm=4.09\n",
      "Iter 74  time=6.40  loss=136.13   feature_norm=4.09\n",
      "Iter 75  time=6.36  loss=131.48   feature_norm=4.12\n",
      "Iter 76  time=6.53  loss=137.33   feature_norm=4.13\n",
      "Iter 77  time=6.39  loss=59.43    feature_norm=4.13\n",
      "Iter 78  time=6.44  loss=95.88    feature_norm=4.15\n",
      "Iter 79  time=6.73  loss=149.91   feature_norm=4.15\n",
      "Iter 80  time=6.43  loss=104.98   feature_norm=4.16\n",
      "Iter 81  time=6.44  loss=127.80   feature_norm=4.16\n",
      "Iter 82  time=6.39  loss=106.97   feature_norm=4.17\n",
      "Iter 83  time=6.50  loss=97.65    feature_norm=4.17\n",
      "Iter 84  time=6.37  loss=136.43   feature_norm=4.18\n",
      "Iter 85  time=6.51  loss=52.92    feature_norm=4.19\n",
      "Iter 86  time=6.58  loss=33.26    feature_norm=4.19\n",
      "Iter 87  time=6.62  loss=29.85    feature_norm=4.19\n",
      "Iter 88  time=6.66  loss=214.16   feature_norm=4.21\n",
      "Iter 89  time=6.58  loss=112.77   feature_norm=4.21\n",
      "Iter 90  time=6.56  loss=108.28   feature_norm=4.21\n",
      "Iter 91  time=6.52  loss=84.96    feature_norm=4.22\n",
      "Iter 92  time=6.58  loss=102.67   feature_norm=4.23\n",
      "Iter 93  time=6.45  loss=173.20   feature_norm=4.24\n",
      "Iter 94  time=6.40  loss=126.62   feature_norm=4.25\n",
      "Iter 95  time=6.35  loss=63.21    feature_norm=4.25\n",
      "Iter 96  time=6.77  loss=102.80   feature_norm=4.26\n",
      "Iter 97  time=6.59  loss=94.16    feature_norm=4.26\n",
      "Iter 98  time=6.62  loss=99.57    feature_norm=4.26\n",
      "Iter 99  time=6.45  loss=183.75   feature_norm=4.27\n",
      "Iter 100 time=6.56  loss=117.28   feature_norm=4.28\n",
      "Total seconds required for training: 658.504\n",
      "\n",
      "Storing the model\n",
      "Number of active features: 73969 (2859236)\n",
      "Number of active attributes: 65317 (2850584)\n",
      "Number of active labels: 2 (2)\n",
      "Writing labels\n",
      "Writing attributes\n",
      "Writing feature references for transitions\n",
      "Writing feature references for attributes\n",
      "Seconds required: 0.183\n",
      "\n",
      "CRF model saved for topic 1250\n",
      "['B', 'B', 'B']\n",
      "['B', 'B', 'B']\n",
      "Precision: 0.9745718728725625, Recall: 0.8372715109954206, F1: 0.9007189268999631\n",
      "Topic 1250 - Avg Precision: 0.9746, Avg Recall: 0.8373, Avg F1-Score: 0.9007\n",
      "for fold: 2\n",
      "label\n",
      "1    33\n",
      "B    33\n",
      "Name: count, dtype: int64 label\n",
      "B    211644\n",
      "1        33\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "1    98\n",
      "B    98\n",
      "Name: count, dtype: int64 label\n",
      "B    719752\n",
      "1        98\n",
      "Name: count, dtype: int64\n",
      "32413173          EXECUTION VERSION\n",
      "32413174    THE BANK OF NOVA SCOTIA\n",
      "Name: sentence, dtype: object\n",
      "First sentence features in training data: [[{'token': 'EXECUTION', 'lower': 'execution', 'is_first': True, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'E', 'prefix-2': 'EX', 'prefix-3': 'EXE', 'suffix-1': 'N', 'suffix-2': 'ON', 'suffix-3': 'ION', 'prev_token': '', 'next_token': 'VERSION', 'is_numeric': False, 'unigram': 'EXECUTION', 'bigram': 'EXECUTION VERSION', 'trigram': ''}, {'token': 'VERSION', 'lower': 'version', 'is_first': False, 'is_last': True, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'V', 'prefix-2': 'VE', 'prefix-3': 'VER', 'suffix-1': 'N', 'suffix-2': 'ON', 'suffix-3': 'ION', 'prev_token': 'EXECUTION', 'next_token': '', 'is_numeric': False, 'unigram': 'VERSION', 'bigram': '', 'trigram': ''}], [{'token': 'THE', 'lower': 'the', 'is_first': True, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'T', 'prefix-2': 'TH', 'prefix-3': 'THE', 'suffix-1': 'E', 'suffix-2': 'HE', 'suffix-3': 'THE', 'prev_token': '', 'next_token': 'BANK', 'is_numeric': False, 'unigram': 'THE', 'bigram': 'THE BANK', 'trigram': 'THE BANK OF'}, {'token': 'BANK', 'lower': 'bank', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'B', 'prefix-2': 'BA', 'prefix-3': 'BAN', 'suffix-1': 'K', 'suffix-2': 'NK', 'suffix-3': 'ANK', 'prev_token': 'THE', 'next_token': 'OF', 'is_numeric': False, 'unigram': 'BANK', 'bigram': 'BANK OF', 'trigram': 'BANK OF NOVA'}, {'token': 'OF', 'lower': 'of', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'O', 'prefix-2': 'OF', 'prefix-3': 'OF', 'suffix-1': 'F', 'suffix-2': 'OF', 'suffix-3': 'OF', 'prev_token': 'BANK', 'next_token': 'NOVA', 'is_numeric': False, 'unigram': 'OF', 'bigram': 'OF NOVA', 'trigram': 'OF NOVA SCOTIA'}, {'token': 'NOVA', 'lower': 'nova', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'N', 'prefix-2': 'NO', 'prefix-3': 'NOV', 'suffix-1': 'A', 'suffix-2': 'VA', 'suffix-3': 'OVA', 'prev_token': 'OF', 'next_token': 'SCOTIA', 'is_numeric': False, 'unigram': 'NOVA', 'bigram': 'NOVA SCOTIA', 'trigram': ''}, {'token': 'SCOTIA', 'lower': 'scotia', 'is_first': False, 'is_last': True, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'S', 'prefix-2': 'SC', 'prefix-3': 'SCO', 'suffix-1': 'A', 'suffix-2': 'IA', 'suffix-3': 'TIA', 'prev_token': 'NOVA', 'next_token': '', 'is_numeric': False, 'unigram': 'SCOTIA', 'bigram': '', 'trigram': ''}]]\n",
      "First sentence labels in training data: [['B', 'B'], ['B', 'B', 'B', 'B', 'B']]\n",
      "Training model for topic 1250 for fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading training data to CRFsuite: 100%|█| 719850/719850 [04:10<00:00, 2872.96it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature generation\n",
      "type: CRF1d\n",
      "feature.minfreq: 0.000000\n",
      "feature.possible_states: 0\n",
      "feature.possible_transitions: 1\n",
      "0....1....2....3....4....5....6....7....8....9....10\n",
      "Number of features: 2768289\n",
      "Seconds required: 66.560\n",
      "\n",
      "Passive Aggressive\n",
      "type: 2\n",
      "c: 0.100000\n",
      "error_sensitive: 1\n",
      "averaging: 1\n",
      "max_iterations: 100\n",
      "epsilon: 0.000000\n",
      "\n",
      "Iter 1   time=6.88  loss=2877.50  feature_norm=1.15\n",
      "Iter 2   time=6.48  loss=1977.74  feature_norm=1.57\n",
      "Iter 3   time=6.39  loss=1688.66  feature_norm=1.87\n",
      "Iter 4   time=6.39  loss=1432.63  feature_norm=2.12\n",
      "Iter 5   time=6.32  loss=1247.31  feature_norm=2.30\n",
      "Iter 6   time=6.43  loss=849.14   feature_norm=2.42\n",
      "Iter 7   time=6.59  loss=779.25   feature_norm=2.50\n",
      "Iter 8   time=6.81  loss=616.31   feature_norm=2.57\n",
      "Iter 9   time=6.35  loss=768.29   feature_norm=2.67\n",
      "Iter 10  time=6.46  loss=653.81   feature_norm=2.75\n",
      "Iter 11  time=6.48  loss=468.06   feature_norm=2.81\n",
      "Iter 12  time=6.22  loss=397.39   feature_norm=2.84\n",
      "Iter 13  time=6.34  loss=309.91   feature_norm=2.88\n",
      "Iter 14  time=6.25  loss=408.33   feature_norm=2.91\n",
      "Iter 15  time=6.33  loss=520.71   feature_norm=2.98\n",
      "Iter 16  time=6.59  loss=409.86   feature_norm=3.01\n",
      "Iter 17  time=6.41  loss=317.52   feature_norm=3.03\n",
      "Iter 18  time=6.65  loss=219.94   feature_norm=3.05\n",
      "Iter 19  time=6.39  loss=428.67   feature_norm=3.09\n",
      "Iter 20  time=6.35  loss=370.63   feature_norm=3.12\n",
      "Iter 21  time=6.32  loss=154.63   feature_norm=3.14\n",
      "Iter 22  time=6.30  loss=284.18   feature_norm=3.18\n",
      "Iter 23  time=6.44  loss=340.13   feature_norm=3.21\n",
      "Iter 24  time=6.48  loss=279.69   feature_norm=3.23\n",
      "Iter 25  time=6.40  loss=318.78   feature_norm=3.26\n",
      "Iter 26  time=6.52  loss=223.80   feature_norm=3.29\n",
      "Iter 27  time=6.45  loss=233.00   feature_norm=3.31\n",
      "Iter 28  time=6.40  loss=218.37   feature_norm=3.32\n",
      "Iter 29  time=6.33  loss=137.92   feature_norm=3.33\n",
      "Iter 30  time=6.46  loss=231.10   feature_norm=3.35\n",
      "Iter 31  time=6.58  loss=169.39   feature_norm=3.37\n",
      "Iter 32  time=6.53  loss=160.03   feature_norm=3.37\n",
      "Iter 33  time=6.74  loss=261.07   feature_norm=3.39\n",
      "Iter 34  time=6.75  loss=217.56   feature_norm=3.41\n",
      "Iter 35  time=6.70  loss=277.29   feature_norm=3.44\n",
      "Iter 36  time=6.46  loss=164.20   feature_norm=3.45\n",
      "Iter 37  time=6.40  loss=201.51   feature_norm=3.47\n",
      "Iter 38  time=6.43  loss=189.42   feature_norm=3.49\n",
      "Iter 39  time=6.40  loss=183.98   feature_norm=3.51\n",
      "Iter 40  time=6.51  loss=312.87   feature_norm=3.54\n",
      "Iter 41  time=6.45  loss=147.97   feature_norm=3.55\n",
      "Iter 42  time=6.58  loss=185.50   feature_norm=3.56\n",
      "Iter 43  time=6.44  loss=115.34   feature_norm=3.57\n",
      "Iter 44  time=6.61  loss=179.93   feature_norm=3.58\n",
      "Iter 45  time=6.56  loss=232.09   feature_norm=3.60\n",
      "Iter 46  time=6.48  loss=215.08   feature_norm=3.61\n",
      "Iter 47  time=6.32  loss=134.66   feature_norm=3.63\n",
      "Iter 48  time=6.34  loss=83.24    feature_norm=3.64\n",
      "Iter 49  time=6.37  loss=199.37   feature_norm=3.64\n",
      "Iter 50  time=6.31  loss=215.33   feature_norm=3.66\n",
      "Iter 51  time=6.40  loss=109.28   feature_norm=3.66\n",
      "Iter 52  time=6.46  loss=94.39    feature_norm=3.67\n",
      "Iter 53  time=6.91  loss=85.84    feature_norm=3.68\n",
      "Iter 54  time=6.35  loss=259.75   feature_norm=3.70\n",
      "Iter 55  time=6.33  loss=192.90   feature_norm=3.71\n",
      "Iter 56  time=6.40  loss=223.12   feature_norm=3.73\n",
      "Iter 57  time=6.39  loss=86.31    feature_norm=3.73\n",
      "Iter 58  time=6.42  loss=80.36    feature_norm=3.74\n",
      "Iter 59  time=6.43  loss=37.67    feature_norm=3.74\n",
      "Iter 60  time=6.37  loss=97.96    feature_norm=3.74\n",
      "Iter 61  time=6.49  loss=174.20   feature_norm=3.76\n",
      "Iter 62  time=6.73  loss=120.07   feature_norm=3.77\n",
      "Iter 63  time=6.44  loss=93.85    feature_norm=3.77\n",
      "Iter 64  time=6.39  loss=187.77   feature_norm=3.79\n",
      "Iter 65  time=6.28  loss=99.67    feature_norm=3.80\n",
      "Iter 66  time=6.66  loss=137.74   feature_norm=3.81\n",
      "Iter 67  time=6.58  loss=58.38    feature_norm=3.81\n",
      "Iter 68  time=6.79  loss=89.49    feature_norm=3.82\n",
      "Iter 69  time=6.73  loss=101.53   feature_norm=3.82\n",
      "Iter 70  time=6.78  loss=88.48    feature_norm=3.83\n",
      "Iter 71  time=6.64  loss=141.62   feature_norm=3.84\n",
      "Iter 72  time=6.55  loss=92.11    feature_norm=3.84\n",
      "Iter 73  time=6.49  loss=86.00    feature_norm=3.85\n",
      "Iter 74  time=6.44  loss=144.17   feature_norm=3.86\n",
      "Iter 75  time=7.21  loss=86.64    feature_norm=3.87\n",
      "Iter 76  time=7.02  loss=106.99   feature_norm=3.87\n",
      "Iter 77  time=6.61  loss=59.06    feature_norm=3.87\n",
      "Iter 78  time=5.77  loss=68.42    feature_norm=3.88\n",
      "Iter 79  time=6.03  loss=91.15    feature_norm=3.88\n",
      "Iter 80  time=5.77  loss=118.75   feature_norm=3.89\n",
      "Iter 81  time=6.03  loss=133.04   feature_norm=3.90\n",
      "Iter 82  time=5.89  loss=47.10    feature_norm=3.90\n",
      "Iter 83  time=5.88  loss=71.29    feature_norm=3.91\n",
      "Iter 84  time=5.66  loss=47.71    feature_norm=3.91\n",
      "Iter 85  time=5.67  loss=22.93    feature_norm=3.91\n",
      "Iter 86  time=6.22  loss=102.05   feature_norm=3.92\n",
      "Iter 87  time=5.60  loss=155.20   feature_norm=3.93\n",
      "Iter 88  time=5.71  loss=74.98    feature_norm=3.94\n",
      "Iter 89  time=6.14  loss=100.75   feature_norm=3.95\n",
      "Iter 90  time=5.74  loss=128.25   feature_norm=3.96\n",
      "Iter 91  time=5.68  loss=139.07   feature_norm=3.97\n",
      "Iter 92  time=5.68  loss=132.39   feature_norm=3.97\n",
      "Iter 93  time=5.65  loss=140.59   feature_norm=3.98\n",
      "Iter 94  time=5.58  loss=135.96   feature_norm=3.99\n",
      "Iter 95  time=5.89  loss=94.08    feature_norm=3.99\n",
      "Iter 96  time=5.78  loss=76.25    feature_norm=4.00\n",
      "Iter 97  time=5.72  loss=39.57    feature_norm=4.00\n",
      "Iter 98  time=5.76  loss=118.99   feature_norm=4.01\n",
      "Iter 99  time=6.54  loss=9.90     feature_norm=4.01\n",
      "Iter 100 time=5.96  loss=96.26    feature_norm=4.01\n",
      "Total seconds required for training: 634.868\n",
      "\n",
      "Storing the model\n",
      "Number of active features: 71975 (2768289)\n",
      "Number of active attributes: 63979 (2760293)\n",
      "Number of active labels: 2 (2)\n",
      "Writing labels\n",
      "Writing attributes\n",
      "Writing feature references for transitions\n",
      "Writing feature references for attributes\n",
      "Seconds required: 0.174\n",
      "\n",
      "CRF model saved for topic 1250\n",
      "['B', 'B']\n",
      "['B', 'B']\n",
      "Precision: 0.9889182052828928, Recall: 0.6721664273055358, F1: 0.8003411791679775\n",
      "Topic 1250 - Avg Precision: 0.9889, Avg Recall: 0.6722, Avg F1-Score: 0.8003\n",
      "for fold: 3\n",
      "label\n",
      "1    26\n",
      "B    26\n",
      "Name: count, dtype: int64 label\n",
      "B    184062\n",
      "1        26\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "1    105\n",
      "B    105\n",
      "Name: count, dtype: int64 label\n",
      "B    747334\n",
      "1       105\n",
      "Name: count, dtype: int64\n",
      "32415991    Exhibit 10.1\n",
      "32415992     J.P. Morgan\n",
      "Name: sentence, dtype: object\n",
      "First sentence features in training data: [[{'token': 'Exhibit', 'lower': 'exhibit', 'is_first': True, 'is_last': False, 'is_capitalized': True, 'is_all_caps': False, 'is_all_lower': False, 'prefix-1': 'E', 'prefix-2': 'Ex', 'prefix-3': 'Exh', 'suffix-1': 't', 'suffix-2': 'it', 'suffix-3': 'bit', 'prev_token': '', 'next_token': '10.1', 'is_numeric': False, 'unigram': 'Exhibit', 'bigram': 'Exhibit 10.1', 'trigram': ''}, {'token': '10.1', 'lower': '10.1', 'is_first': False, 'is_last': True, 'is_capitalized': False, 'is_all_caps': False, 'is_all_lower': False, 'prefix-1': '1', 'prefix-2': '10', 'prefix-3': '10.', 'suffix-1': '1', 'suffix-2': '.1', 'suffix-3': '0.1', 'prev_token': 'Exhibit', 'next_token': '', 'is_numeric': False, 'unigram': '10.1', 'bigram': '', 'trigram': ''}], [{'token': 'J.P.', 'lower': 'j.p.', 'is_first': True, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'J', 'prefix-2': 'J.', 'prefix-3': 'J.P', 'suffix-1': '.', 'suffix-2': 'P.', 'suffix-3': '.P.', 'prev_token': '', 'next_token': 'Morgan', 'is_numeric': False, 'unigram': 'J.P.', 'bigram': 'J.P. Morgan', 'trigram': ''}, {'token': 'Morgan', 'lower': 'morgan', 'is_first': False, 'is_last': True, 'is_capitalized': True, 'is_all_caps': False, 'is_all_lower': False, 'prefix-1': 'M', 'prefix-2': 'Mo', 'prefix-3': 'Mor', 'suffix-1': 'n', 'suffix-2': 'an', 'suffix-3': 'gan', 'prev_token': 'J.P.', 'next_token': '', 'is_numeric': False, 'unigram': 'Morgan', 'bigram': '', 'trigram': ''}]]\n",
      "First sentence labels in training data: [['B', 'B'], ['B', 'B']]\n",
      "Training model for topic 1250 for fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading training data to CRFsuite: 100%|█| 747439/747439 [04:05<00:00, 3048.99it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature generation\n",
      "type: CRF1d\n",
      "feature.minfreq: 0.000000\n",
      "feature.possible_states: 0\n",
      "feature.possible_transitions: 1\n",
      "0....1....2....3....4....5....6....7....8....9....10\n",
      "Number of features: 2796712\n",
      "Seconds required: 61.890\n",
      "\n",
      "Passive Aggressive\n",
      "type: 2\n",
      "c: 0.100000\n",
      "error_sensitive: 1\n",
      "averaging: 1\n",
      "max_iterations: 100\n",
      "epsilon: 0.000000\n",
      "\n",
      "Iter 1   time=7.13  loss=3012.18  feature_norm=1.18\n",
      "Iter 2   time=6.83  loss=2126.32  feature_norm=1.62\n",
      "Iter 3   time=6.75  loss=1785.39  feature_norm=1.94\n",
      "Iter 4   time=6.29  loss=1641.70  feature_norm=2.19\n",
      "Iter 5   time=6.40  loss=1419.06  feature_norm=2.41\n",
      "Iter 6   time=6.48  loss=1108.09  feature_norm=2.52\n",
      "Iter 7   time=6.92  loss=816.65   feature_norm=2.62\n",
      "Iter 8   time=6.77  loss=767.93   feature_norm=2.72\n",
      "Iter 9   time=6.70  loss=695.41   feature_norm=2.80\n",
      "Iter 10  time=7.29  loss=580.39   feature_norm=2.84\n",
      "Iter 11  time=6.52  loss=721.76   feature_norm=2.90\n",
      "Iter 12  time=6.59  loss=514.17   feature_norm=2.95\n",
      "Iter 13  time=6.66  loss=552.63   feature_norm=3.00\n",
      "Iter 14  time=6.50  loss=626.70   feature_norm=3.06\n",
      "Iter 15  time=6.48  loss=570.08   feature_norm=3.12\n",
      "Iter 16  time=6.88  loss=379.41   feature_norm=3.15\n",
      "Iter 17  time=6.62  loss=312.42   feature_norm=3.18\n",
      "Iter 18  time=6.63  loss=420.17   feature_norm=3.22\n",
      "Iter 19  time=7.10  loss=395.80   feature_norm=3.25\n",
      "Iter 20  time=6.75  loss=323.42   feature_norm=3.28\n",
      "Iter 21  time=6.87  loss=427.81   feature_norm=3.32\n",
      "Iter 22  time=6.70  loss=420.83   feature_norm=3.37\n",
      "Iter 23  time=6.61  loss=306.74   feature_norm=3.40\n",
      "Iter 24  time=6.30  loss=250.74   feature_norm=3.41\n",
      "Iter 25  time=6.29  loss=208.48   feature_norm=3.43\n",
      "Iter 26  time=6.75  loss=561.78   feature_norm=3.48\n",
      "Iter 27  time=7.04  loss=378.97   feature_norm=3.50\n",
      "Iter 28  time=6.42  loss=320.42   feature_norm=3.52\n",
      "Iter 29  time=6.55  loss=273.74   feature_norm=3.54\n",
      "Iter 30  time=6.48  loss=271.39   feature_norm=3.55\n",
      "Iter 31  time=6.42  loss=303.12   feature_norm=3.57\n",
      "Iter 32  time=6.55  loss=266.43   feature_norm=3.59\n",
      "Iter 33  time=6.45  loss=208.22   feature_norm=3.60\n",
      "Iter 34  time=6.49  loss=276.69   feature_norm=3.62\n",
      "Iter 35  time=6.41  loss=310.04   feature_norm=3.64\n",
      "Iter 36  time=6.89  loss=218.24   feature_norm=3.66\n",
      "Iter 37  time=6.55  loss=208.92   feature_norm=3.67\n",
      "Iter 38  time=6.53  loss=243.79   feature_norm=3.69\n",
      "Iter 39  time=6.41  loss=119.51   feature_norm=3.70\n",
      "Iter 40  time=6.57  loss=100.89   feature_norm=3.72\n",
      "Iter 41  time=6.54  loss=148.53   feature_norm=3.73\n",
      "Iter 42  time=6.48  loss=151.54   feature_norm=3.75\n",
      "Iter 43  time=6.53  loss=275.27   feature_norm=3.77\n",
      "Iter 44  time=6.56  loss=241.31   feature_norm=3.79\n",
      "Iter 45  time=6.77  loss=234.23   feature_norm=3.81\n",
      "Iter 46  time=6.37  loss=143.34   feature_norm=3.82\n",
      "Iter 47  time=6.36  loss=165.73   feature_norm=3.83\n",
      "Iter 48  time=6.93  loss=218.02   feature_norm=3.83\n",
      "Iter 49  time=6.69  loss=236.71   feature_norm=3.86\n",
      "Iter 50  time=6.36  loss=189.34   feature_norm=3.88\n",
      "Iter 51  time=6.35  loss=166.43   feature_norm=3.88\n",
      "Iter 52  time=6.37  loss=112.25   feature_norm=3.90\n",
      "Iter 53  time=6.58  loss=245.77   feature_norm=3.92\n",
      "Iter 54  time=7.04  loss=177.66   feature_norm=3.94\n",
      "Iter 55  time=6.65  loss=197.80   feature_norm=3.95\n",
      "Iter 56  time=6.62  loss=234.42   feature_norm=3.95\n",
      "Iter 57  time=6.55  loss=150.87   feature_norm=3.96\n",
      "Iter 58  time=6.38  loss=99.22    feature_norm=3.96\n",
      "Iter 59  time=6.50  loss=69.56    feature_norm=3.97\n",
      "Iter 60  time=6.39  loss=129.18   feature_norm=3.97\n",
      "Iter 61  time=6.39  loss=115.28   feature_norm=3.98\n",
      "Iter 62  time=7.01  loss=96.88    feature_norm=3.98\n",
      "Iter 63  time=6.48  loss=113.02   feature_norm=3.99\n",
      "Iter 64  time=6.48  loss=116.19   feature_norm=3.99\n",
      "Iter 65  time=6.44  loss=137.92   feature_norm=4.00\n",
      "Iter 66  time=6.42  loss=168.75   feature_norm=4.01\n",
      "Iter 67  time=6.39  loss=69.21    feature_norm=4.01\n",
      "Iter 68  time=6.42  loss=122.98   feature_norm=4.02\n",
      "Iter 69  time=6.60  loss=207.24   feature_norm=4.04\n",
      "Iter 70  time=6.38  loss=130.03   feature_norm=4.05\n",
      "Iter 71  time=6.51  loss=57.95    feature_norm=4.06\n",
      "Iter 72  time=6.42  loss=127.13   feature_norm=4.06\n",
      "Iter 73  time=6.47  loss=58.73    feature_norm=4.07\n",
      "Iter 74  time=6.41  loss=86.13    feature_norm=4.08\n",
      "Iter 75  time=6.41  loss=202.46   feature_norm=4.09\n",
      "Iter 76  time=6.65  loss=109.41   feature_norm=4.10\n",
      "Iter 77  time=6.42  loss=55.80    feature_norm=4.09\n",
      "Iter 78  time=6.54  loss=218.34   feature_norm=4.10\n",
      "Iter 79  time=6.42  loss=151.44   feature_norm=4.12\n",
      "Iter 80  time=7.00  loss=151.49   feature_norm=4.13\n",
      "Iter 81  time=6.37  loss=99.73    feature_norm=4.14\n",
      "Iter 82  time=6.42  loss=89.47    feature_norm=4.14\n",
      "Iter 83  time=6.38  loss=79.49    feature_norm=4.15\n",
      "Iter 84  time=6.50  loss=112.40   feature_norm=4.15\n",
      "Iter 85  time=6.48  loss=139.34   feature_norm=4.16\n",
      "Iter 86  time=6.44  loss=159.50   feature_norm=4.16\n",
      "Iter 87  time=7.26  loss=118.59   feature_norm=4.17\n",
      "Iter 88  time=6.69  loss=128.97   feature_norm=4.19\n",
      "Iter 89  time=6.73  loss=88.24    feature_norm=4.19\n",
      "Iter 90  time=6.34  loss=151.45   feature_norm=4.20\n",
      "Iter 91  time=6.43  loss=137.29   feature_norm=4.21\n",
      "Iter 92  time=9.55  loss=61.44    feature_norm=4.21\n",
      "Iter 93  time=7.58  loss=57.75    feature_norm=4.21\n",
      "Iter 94  time=6.57  loss=116.49   feature_norm=4.22\n",
      "Iter 95  time=6.61  loss=123.47   feature_norm=4.23\n",
      "Iter 96  time=6.92  loss=32.41    feature_norm=4.23\n",
      "Iter 97  time=6.73  loss=115.05   feature_norm=4.23\n",
      "Iter 98  time=6.69  loss=90.90    feature_norm=4.24\n",
      "Iter 99  time=6.73  loss=93.43    feature_norm=4.25\n",
      "Iter 100 time=6.61  loss=74.46    feature_norm=4.25\n",
      "Total seconds required for training: 662.831\n",
      "\n",
      "Storing the model\n",
      "Number of active features: 73965 (2796712)\n",
      "Number of active attributes: 65586 (2788333)\n",
      "Number of active labels: 2 (2)\n",
      "Writing labels\n",
      "Writing attributes\n",
      "Writing feature references for transitions\n",
      "Writing feature references for attributes\n",
      "Seconds required: 0.181\n",
      "\n",
      "CRF model saved for topic 1250\n",
      "['B', 'B']\n",
      "['B', 'B']\n",
      "Precision: 0.9999999994044073, Recall: 0.7525773192503016, F1: 0.8588230389381464\n",
      "Topic 1250 - Avg Precision: 1.0000, Avg Recall: 0.7526, Avg F1-Score: 0.8588\n",
      "for fold: 4\n",
      "label\n",
      "1    21\n",
      "B    21\n",
      "Name: count, dtype: int64 label\n",
      "B    170880\n",
      "1        21\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "1    110\n",
      "B    110\n",
      "Name: count, dtype: int64 label\n",
      "B    760516\n",
      "1       110\n",
      "Name: count, dtype: int64\n",
      "32413173          EXECUTION VERSION\n",
      "32413174    THE BANK OF NOVA SCOTIA\n",
      "Name: sentence, dtype: object\n",
      "First sentence features in training data: [[{'token': 'EXECUTION', 'lower': 'execution', 'is_first': True, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'E', 'prefix-2': 'EX', 'prefix-3': 'EXE', 'suffix-1': 'N', 'suffix-2': 'ON', 'suffix-3': 'ION', 'prev_token': '', 'next_token': 'VERSION', 'is_numeric': False, 'unigram': 'EXECUTION', 'bigram': 'EXECUTION VERSION', 'trigram': ''}, {'token': 'VERSION', 'lower': 'version', 'is_first': False, 'is_last': True, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'V', 'prefix-2': 'VE', 'prefix-3': 'VER', 'suffix-1': 'N', 'suffix-2': 'ON', 'suffix-3': 'ION', 'prev_token': 'EXECUTION', 'next_token': '', 'is_numeric': False, 'unigram': 'VERSION', 'bigram': '', 'trigram': ''}], [{'token': 'THE', 'lower': 'the', 'is_first': True, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'T', 'prefix-2': 'TH', 'prefix-3': 'THE', 'suffix-1': 'E', 'suffix-2': 'HE', 'suffix-3': 'THE', 'prev_token': '', 'next_token': 'BANK', 'is_numeric': False, 'unigram': 'THE', 'bigram': 'THE BANK', 'trigram': 'THE BANK OF'}, {'token': 'BANK', 'lower': 'bank', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'B', 'prefix-2': 'BA', 'prefix-3': 'BAN', 'suffix-1': 'K', 'suffix-2': 'NK', 'suffix-3': 'ANK', 'prev_token': 'THE', 'next_token': 'OF', 'is_numeric': False, 'unigram': 'BANK', 'bigram': 'BANK OF', 'trigram': 'BANK OF NOVA'}, {'token': 'OF', 'lower': 'of', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'O', 'prefix-2': 'OF', 'prefix-3': 'OF', 'suffix-1': 'F', 'suffix-2': 'OF', 'suffix-3': 'OF', 'prev_token': 'BANK', 'next_token': 'NOVA', 'is_numeric': False, 'unigram': 'OF', 'bigram': 'OF NOVA', 'trigram': 'OF NOVA SCOTIA'}, {'token': 'NOVA', 'lower': 'nova', 'is_first': False, 'is_last': False, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'N', 'prefix-2': 'NO', 'prefix-3': 'NOV', 'suffix-1': 'A', 'suffix-2': 'VA', 'suffix-3': 'OVA', 'prev_token': 'OF', 'next_token': 'SCOTIA', 'is_numeric': False, 'unigram': 'NOVA', 'bigram': 'NOVA SCOTIA', 'trigram': ''}, {'token': 'SCOTIA', 'lower': 'scotia', 'is_first': False, 'is_last': True, 'is_capitalized': True, 'is_all_caps': True, 'is_all_lower': False, 'prefix-1': 'S', 'prefix-2': 'SC', 'prefix-3': 'SCO', 'suffix-1': 'A', 'suffix-2': 'IA', 'suffix-3': 'TIA', 'prev_token': 'NOVA', 'next_token': '', 'is_numeric': False, 'unigram': 'SCOTIA', 'bigram': '', 'trigram': ''}]]\n",
      "First sentence labels in training data: [['B', 'B'], ['B', 'B', 'B', 'B', 'B']]\n",
      "Training model for topic 1250 for fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading training data to CRFsuite: 100%|█| 760626/760626 [04:31<00:00, 2796.48it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature generation\n",
      "type: CRF1d\n",
      "feature.minfreq: 0.000000\n",
      "feature.possible_states: 0\n",
      "feature.possible_transitions: 1\n",
      "0....1....2....3....4....5....6....7....8....9....10\n",
      "Number of features: 2854799\n",
      "Seconds required: 65.810\n",
      "\n",
      "Passive Aggressive\n",
      "type: 2\n",
      "c: 0.100000\n",
      "error_sensitive: 1\n",
      "averaging: 1\n",
      "max_iterations: 100\n",
      "epsilon: 0.000000\n",
      "\n",
      "Iter 1   time=7.86  loss=3106.78  feature_norm=1.18\n",
      "Iter 2   time=7.47  loss=2394.60  feature_norm=1.64\n",
      "Iter 3   time=7.15  loss=1844.72  feature_norm=1.94\n",
      "Iter 4   time=7.62  loss=1666.57  feature_norm=2.13\n",
      "Iter 5   time=7.22  loss=1261.67  feature_norm=2.32\n",
      "Iter 6   time=7.15  loss=1150.85  feature_norm=2.48\n",
      "Iter 7   time=7.05  loss=932.07   feature_norm=2.60\n",
      "Iter 8   time=6.99  loss=981.41   feature_norm=2.69\n",
      "Iter 9   time=6.96  loss=567.79   feature_norm=2.75\n",
      "Iter 10  time=7.11  loss=694.05   feature_norm=2.82\n",
      "Iter 11  time=10.29 loss=411.88   feature_norm=2.85\n",
      "Iter 12  time=8.21  loss=439.27   feature_norm=2.89\n",
      "Iter 13  time=7.05  loss=529.47   feature_norm=2.94\n",
      "Iter 14  time=7.06  loss=662.78   feature_norm=3.01\n",
      "Iter 15  time=6.97  loss=558.68   feature_norm=3.06\n",
      "Iter 16  time=7.02  loss=231.77   feature_norm=3.07\n",
      "Iter 17  time=7.33  loss=559.75   feature_norm=3.12\n",
      "Iter 18  time=7.19  loss=280.43   feature_norm=3.15\n",
      "Iter 19  time=7.20  loss=474.88   feature_norm=3.19\n",
      "Iter 20  time=7.17  loss=186.67   feature_norm=3.22\n",
      "Iter 21  time=7.41  loss=250.01   feature_norm=3.23\n",
      "Iter 22  time=7.18  loss=293.64   feature_norm=3.26\n",
      "Iter 23  time=7.38  loss=289.67   feature_norm=3.29\n",
      "Iter 24  time=7.01  loss=407.37   feature_norm=3.33\n",
      "Iter 25  time=7.23  loss=312.15   feature_norm=3.34\n",
      "Iter 26  time=7.24  loss=256.62   feature_norm=3.36\n",
      "Iter 27  time=7.56  loss=378.32   feature_norm=3.39\n",
      "Iter 28  time=7.25  loss=211.30   feature_norm=3.42\n",
      "Iter 29  time=7.66  loss=106.81   feature_norm=3.42\n",
      "Iter 30  time=7.16  loss=212.07   feature_norm=3.44\n",
      "Iter 31  time=7.28  loss=267.25   feature_norm=3.46\n",
      "Iter 32  time=7.19  loss=163.83   feature_norm=3.47\n",
      "Iter 33  time=7.14  loss=91.60    feature_norm=3.47\n",
      "Iter 34  time=7.46  loss=262.39   feature_norm=3.49\n",
      "Iter 35  time=7.52  loss=196.45   feature_norm=3.49\n",
      "Iter 36  time=7.20  loss=24.69    feature_norm=3.50\n",
      "Iter 37  time=7.12  loss=30.31    feature_norm=3.50\n",
      "Iter 38  time=7.04  loss=156.07   feature_norm=3.51\n",
      "Iter 39  time=7.13  loss=176.71   feature_norm=3.52\n",
      "Iter 40  time=7.08  loss=59.52    feature_norm=3.52\n",
      "Iter 41  time=7.16  loss=207.31   feature_norm=3.54\n",
      "Iter 42  time=7.33  loss=205.16   feature_norm=3.56\n",
      "Iter 43  time=7.41  loss=54.32    feature_norm=3.56\n",
      "Iter 44  time=7.28  loss=188.32   feature_norm=3.58\n",
      "Iter 45  time=7.24  loss=130.32   feature_norm=3.59\n",
      "Iter 46  time=7.19  loss=128.94   feature_norm=3.60\n",
      "Iter 47  time=7.08  loss=197.75   feature_norm=3.61\n",
      "Iter 48  time=7.08  loss=175.60   feature_norm=3.62\n",
      "Iter 49  time=7.12  loss=64.97    feature_norm=3.62\n",
      "Iter 50  time=7.35  loss=69.35    feature_norm=3.63\n",
      "Iter 51  time=7.05  loss=87.24    feature_norm=3.64\n",
      "Iter 52  time=7.12  loss=121.56   feature_norm=3.65\n",
      "Iter 53  time=7.13  loss=153.72   feature_norm=3.66\n",
      "Iter 54  time=7.39  loss=121.09   feature_norm=3.67\n",
      "Iter 55  time=7.11  loss=151.15   feature_norm=3.68\n",
      "Iter 56  time=6.94  loss=151.48   feature_norm=3.69\n",
      "Iter 57  time=6.99  loss=166.98   feature_norm=3.71\n",
      "Iter 58  time=7.61  loss=222.89   feature_norm=3.73\n",
      "Iter 59  time=7.04  loss=35.24    feature_norm=3.73\n",
      "Iter 60  time=7.38  loss=95.49    feature_norm=3.74\n",
      "Iter 61  time=7.21  loss=215.16   feature_norm=3.75\n",
      "Iter 62  time=7.44  loss=108.56   feature_norm=3.76\n",
      "Iter 63  time=7.43  loss=106.90   feature_norm=3.77\n",
      "Iter 64  time=7.07  loss=58.90    feature_norm=3.77\n",
      "Iter 65  time=7.13  loss=77.89    feature_norm=3.78\n",
      "Iter 66  time=7.51  loss=179.32   feature_norm=3.79\n",
      "Iter 67  time=7.37  loss=85.50    feature_norm=3.79\n",
      "Iter 68  time=7.13  loss=25.96    feature_norm=3.79\n",
      "Iter 69  time=7.12  loss=60.56    feature_norm=3.79\n",
      "Iter 70  time=7.13  loss=208.35   feature_norm=3.81\n",
      "Iter 71  time=7.47  loss=147.96   feature_norm=3.82\n",
      "Iter 72  time=7.01  loss=183.17   feature_norm=3.84\n",
      "Iter 73  time=7.24  loss=73.38    feature_norm=3.85\n",
      "Iter 74  time=7.38  loss=29.26    feature_norm=3.85\n",
      "Iter 75  time=7.29  loss=67.57    feature_norm=3.86\n",
      "Iter 76  time=7.42  loss=279.81   feature_norm=3.87\n",
      "Iter 77  time=7.14  loss=42.76    feature_norm=3.88\n",
      "Iter 78  time=7.18  loss=87.75    feature_norm=3.89\n",
      "Iter 79  time=7.06  loss=75.98    feature_norm=3.89\n",
      "Iter 80  time=7.04  loss=34.34    feature_norm=3.89\n",
      "Iter 81  time=7.25  loss=98.21    feature_norm=3.89\n",
      "Iter 82  time=7.29  loss=124.69   feature_norm=3.90\n",
      "Iter 83  time=7.24  loss=55.73    feature_norm=3.91\n",
      "Iter 84  time=7.14  loss=96.99    feature_norm=3.92\n",
      "Iter 85  time=7.11  loss=38.50    feature_norm=3.91\n",
      "Iter 86  time=6.96  loss=28.12    feature_norm=3.91\n",
      "Iter 87  time=7.20  loss=22.18    feature_norm=3.92\n",
      "Iter 88  time=6.99  loss=61.95    feature_norm=3.92\n",
      "Iter 89  time=7.32  loss=25.90    feature_norm=3.92\n",
      "Iter 90  time=7.32  loss=66.86    feature_norm=3.92\n",
      "Iter 91  time=7.13  loss=44.52    feature_norm=3.93\n",
      "Iter 92  time=7.23  loss=49.61    feature_norm=3.93\n",
      "Iter 93  time=7.20  loss=23.19    feature_norm=3.93\n",
      "Iter 94  time=7.37  loss=161.77   feature_norm=3.94\n",
      "Iter 95  time=7.42  loss=130.53   feature_norm=3.95\n",
      "Iter 96  time=7.34  loss=113.56   feature_norm=3.96\n",
      "Iter 97  time=7.32  loss=47.33    feature_norm=3.96\n",
      "Iter 98  time=7.20  loss=98.24    feature_norm=3.97\n",
      "Iter 99  time=7.28  loss=84.60    feature_norm=3.97\n",
      "Iter 100 time=7.52  loss=16.28    feature_norm=3.97\n",
      "Total seconds required for training: 726.958\n",
      "\n",
      "Storing the model\n",
      "Number of active features: 75624 (2854799)\n",
      "Number of active attributes: 66947 (2846122)\n",
      "Number of active labels: 2 (2)\n",
      "Writing labels\n",
      "Writing attributes\n",
      "Writing feature references for transitions\n",
      "Writing feature references for attributes\n",
      "Seconds required: 0.183\n",
      "\n",
      "CRF model saved for topic 1250\n",
      "['B', 'B', 'B', 'B']\n",
      "['B', 'B', 'B', 'B']\n",
      "Precision: 0.9351432873792282, Recall: 0.7612031916751361, F1: 0.8392550038432719\n",
      "Topic 1250 - Avg Precision: 0.9351, Avg Recall: 0.7612, Avg F1-Score: 0.8393\n",
      "Saving the prediction\n",
      "Precision: 0.9789322479077169, Recall: 0.7715094338894802, F1: 0.8629308143057968\n",
      "Sentence level results\n",
      "Topic 1250 - Avg Precision: 0.9789, Avg Recall: 0.7715, Avg F1-Score: 0.8629\n",
      "81\n",
      "66\n",
      "{'TP': 63, 'FP': 6, 'FN': 3, 'Precision': 0.9130434650283556, 'Recall': 0.9545454400826449, 'F1-Score': 0.9333328197533544}\n",
      "Results saved to raw_data_exp_25_04_24/1250/results.csv\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store predictions and labels\n",
    "\n",
    "# Iterate over folders\n",
    "for folder in folders_to_process:\n",
    "\n",
    "    if folder in os.listdir(path):  # Check if the folder is actually in the directory\n",
    "        for fold in range(5):\n",
    "            \n",
    "            ####### Data split into train and test sets #######\n",
    "            print(\"for fold:\", fold)\n",
    "            test_split = fold\n",
    "            train_split = [i for i in range(5) if i != test_split]\n",
    "            \n",
    "            test = read_split(f'{path}/{folder}/{folder}-{test_split}.cache')\n",
    "            train = sum([read_split(f'{path}/{folder}/{folder}-{el}.cache') for el in train_split], [])\n",
    "\n",
    "            df_ = df[df['topic_id'] == int(folder)]\n",
    "            df_ = df_.dropna()\n",
    "            df_['doc_id'] = df_.apply(map_doc, axis=1)\n",
    "\n",
    "            df_test = df_[df_['doc_id'].isin(test)]\n",
    "            df_train = df_[df_['doc_id'].isin(train)]\n",
    "\n",
    "            test_unbalanced, test_balanced = stratify(df_test)\n",
    "            train_unbalanced, train_balanced = stratify(df_train)\n",
    "            \n",
    "            print(test_balanced['label'].value_counts(), test_unbalanced['label'].value_counts())\n",
    "            print(train_balanced['label'].value_counts(), train_unbalanced['label'].value_counts())\n",
    "            \n",
    "            ######### Model training ##############\n",
    "\n",
    "            model_save_dir = 'raw_data_exp_25_04_24'\n",
    "            os.makedirs(model_save_dir, exist_ok=True)\n",
    "            \n",
    "            ## train CRF model\n",
    "            train_crf_model(train_unbalanced, test_unbalanced, model_save_dir, str(int(folder)), fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27f4548",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bea9c6e",
   "metadata": {},
   "source": [
    "### Sentence level eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ba401d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_level_results(true_labels_flat, predicted_labels_flat):\n",
    "    \n",
    "    tp, fp, fn = 0, 0, 0\n",
    "    for gold, pred in zip(true_labels_flat, predicted_labels_flat):\n",
    "        # Skip empty strings\n",
    "        if gold == '' or pred == '':\n",
    "            continue\n",
    "        gold = 0 if gold == 'B' else int(gold)\n",
    "        pred = 0 if pred == 'B' else int(pred)\n",
    "\n",
    "        if gold == 1 and pred == 1:\n",
    "            tp += 1\n",
    "        elif gold != 1 and pred == 1:\n",
    "            fp += 1\n",
    "        elif gold == 1 and pred != 1:\n",
    "            fn += 1\n",
    "\n",
    "    # Compute metrics\n",
    "    eps = 1e-6\n",
    "    precision = tp / (tp + fp + eps)\n",
    "    recall = tp / (tp + fn + eps)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + eps)\n",
    "\n",
    "    print(f\"Precision: {precision}, Recall: {recall}, F1: {f1}\")\n",
    "    \n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b5ed42",
   "metadata": {},
   "source": [
    "### Annotation level eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7934d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_to_spans(labels):\n",
    "    \"\"\"\n",
    "    Converts a list of labels ('1' or 'B') into spans.\n",
    "    \n",
    "    Args:\n",
    "    - labels: List of labels ('1' or 'B') for tokens.\n",
    "\n",
    "    Returns:\n",
    "    - A list of spans represented as tuples (start, end).\n",
    "    \"\"\"\n",
    "    start = None\n",
    "    spans = []\n",
    "    for pos, label in enumerate(labels):\n",
    "        if label == \"1\" and start is None:  # Start of a new span\n",
    "            start = pos\n",
    "        elif label != \"1\" and start is not None:  # End of the current span\n",
    "            spans.append((start, pos - 1))\n",
    "            start = None\n",
    "    if start is not None:  # If a span extends to the end of the sequence\n",
    "        spans.append((start, len(labels) - 1))\n",
    "        \n",
    "    print(len(spans))\n",
    "        \n",
    "    return spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab881aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels_and_create_spans(file_path, output_file):\n",
    "    with open(file_path, 'r') as f:\n",
    "        raw_data = f.readlines()\n",
    "        \n",
    "    # Initialize variables\n",
    "    current_sequence = []\n",
    "    all_spans = []\n",
    "    \n",
    "    for line in raw_data:\n",
    "        label = line.strip()\n",
    "        if label:  # If the line is not empty, add the label to the current sequence\n",
    "            current_sequence.append(label)\n",
    "        else:  # If the line is empty, process the current sequence and reset it\n",
    "            if current_sequence:  # Check if the current sequence is not empty\n",
    "                spans = labels_to_spans(current_sequence)\n",
    "                all_spans.extend(spans)\n",
    "                current_sequence = []  # Reset the sequence for the next block\n",
    "\n",
    "    # Process the last sequence if the file doesn't end with a blank line\n",
    "    if current_sequence:\n",
    "        spans = labels_to_spans(current_sequence)\n",
    "        all_spans.extend(spans)\n",
    "    \n",
    "    # Write the spans to the output file\n",
    "    with open(output_file, 'w') as f:\n",
    "        for span in all_spans:\n",
    "            f.write(f\"{span[0]} {span[1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3aa66235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_span_file(filename):\n",
    "    \"\"\"Parses a span file to extract spans.\"\"\"\n",
    "    spans = []\n",
    "    with open(filename) as fil:\n",
    "        for line in fil:\n",
    "            start, end = map(int, line.strip().split())\n",
    "            spans.append({'start': start, 'end': 1 + end})\n",
    "    return spans\n",
    "\n",
    "def span_overlaps(A, B):\n",
    "    \"\"\"Checks if two spans overlap.\"\"\"\n",
    "    return not ((A['end'] <= B['start']) or (A['start'] >= B['end']))\n",
    "\n",
    "def overlapping_spans(A, B):\n",
    "    \"\"\"Finds overlapping spans between two lists of spans.\"\"\"\n",
    "    return [a for a in A if any(span_overlaps(a, b) for b in B)]\n",
    "\n",
    "def non_overlapping_spans(A, B):\n",
    "    \"\"\"Finds spans in A that do not overlap with any span in B.\"\"\"\n",
    "    return [a for a in A if not any(span_overlaps(a, b) for b in B)]\n",
    "\n",
    "def evaluate_annotations(gold_file, pred_file):\n",
    "    \"\"\"Evaluates the predicted annotations against the gold standard.\"\"\"\n",
    "    gold = parse_span_file(gold_file)\n",
    "    pred = parse_span_file(pred_file)\n",
    "    \n",
    "    eps = 0.000001\n",
    "    tp = len(overlapping_spans(gold, pred))\n",
    "    fp = len(non_overlapping_spans(pred, gold))\n",
    "    fn = len(non_overlapping_spans(gold, pred))\n",
    "    \n",
    "    recall = tp / float(tp + fn + eps)\n",
    "    precision = tp / float(tp + fp + eps)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + eps)\n",
    "    \n",
    "    return {\"TP\": tp, \"FP\": fp, \"FN\": fn, \"Precision\": precision, \"Recall\": recall, \"F1-Score\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d56cac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from itertools import chain\n",
    "import os\n",
    "import sklearn_crfsuite\n",
    "\n",
    "def load_crf_model(model_path):\n",
    "    \"\"\"Load a saved CRF model from a file.\"\"\"\n",
    "    with open(model_path, 'rb') as model_file:\n",
    "        crf_model = pickle.load(model_file)\n",
    "    return crf_model\n",
    "\n",
    "def predict_with_crf(crf_model, X_test):\n",
    "    \"\"\"Predict using a loaded CRF model.\"\"\"\n",
    "    return crf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4b5be3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_predictions(y_test, y_pred, fold, results_df, model_save_dir, folder):\n",
    "    \"\"\"Evaluate the model predictions using appropriate metrics.\"\"\"\n",
    "    \n",
    "\n",
    "    topic_id = folder\n",
    "    y_test_flat = list(chain.from_iterable(y_test))\n",
    "    y_pred_flat = list(chain.from_iterable(y_pred))\n",
    "    \n",
    "    # Save the combined predictions and gold labels for the topic\n",
    "    pred_file_path = os.path.join(model_save_dir, topic_id, f\"{topic_id}_{fold}.pred.raw\")\n",
    "    gold_file_path = os.path.join(model_save_dir, topic_id, f\"{topic_id}_{fold}.gold.raw\")\n",
    "\n",
    "    with open(pred_file_path, \"w\") as pred_file, open(gold_file_path, \"w\") as gold_file:\n",
    "        pred_file.write(\"\\n\".join(y_test_flat))\n",
    "        gold_file.write(\"\\n\".join(y_pred_flat))\n",
    "    \n",
    "    precision, recall, f1_score = sentence_level_results(y_test_flat, y_pred_flat)\n",
    "    print(f\"Avg Precision: {precision:.4f}, Avg Recall: {recall:.4f}, Avg F1-Score: {f1_score:.4f}\")\n",
    "    \n",
    "    # Create spans and save to .span files\n",
    "    load_labels_and_create_spans(pred_file_path, os.path.join(model_save_dir, topic_id, f\"{topic_id}_{fold}.pred.span\"))\n",
    "    load_labels_and_create_spans(gold_file_path, os.path.join(model_save_dir, topic_id, f\"{topic_id}_{fold}.gold.span\"))\n",
    "\n",
    "    # Evaluate the annotations\n",
    "    metrics = evaluate_annotations(os.path.join(model_save_dir, topic_id, f\"{topic_id}_{fold}.gold.span\"), os.path.join(model_save_dir, topic_id, f\"{topic_id}_{fold}.pred.span\"))\n",
    "    print(metrics)\n",
    "    \n",
    "    new_row = pd.DataFrame([{\n",
    "        \"Topic ID\": topic_id,\n",
    "        \"Fold\": fold,\n",
    "        \"Sentence Precision\": precision,\n",
    "        \"Sentence Recall\": recall,\n",
    "        \"Sentence F1-Score\": f1_score,\n",
    "        \"Annotation TP\": metrics[\"TP\"],\n",
    "        \"Annotation FP\": metrics[\"FP\"],\n",
    "        \"Annotation FN\": metrics[\"FN\"],\n",
    "        \"Annotation Precision\": metrics[\"Precision\"],\n",
    "        \"Annotation Recall\": metrics[\"Recall\"],\n",
    "        \"Annotation F1-Score\": metrics[\"F1-Score\"],\n",
    "    }], columns=results_df.columns)\n",
    "\n",
    "    results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "77a5d989",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_crf_model(model_save_dir, folders_to_process, path, df):\n",
    "    for folder in folders_to_process:\n",
    "        \n",
    "        if folder in os.listdir(path):\n",
    "            \n",
    "            ## Dataframe to store the results\n",
    "            results_df = pd.DataFrame(columns=[\"Topic ID\", \"Fold\", \"Sentence Precision\", \"Sentence Recall\", \"Sentence F1-Score\",\n",
    "                                       \"Annotation TP\", \"Annotation FP\", \"Annotation FN\", \n",
    "                                       \"Annotation Precision\", \"Annotation Recall\", \"Annotation F1-Score\"])\n",
    "            for fold in range(5):\n",
    "                print(f\"Processing topic {folder}, fold {fold}\")\n",
    "                ### Load the saved model for the topic and fold\n",
    "                fold_model_path = os.path.join(model_save_dir, folder, f\"{folder}_crf_model_{fold}.pkl\")\n",
    "                if os.path.exists(fold_model_path):\n",
    "                    crf_model = load_crf_model(fold_model_path)\n",
    "                    \n",
    "                    # Load test data splits\n",
    "                    test = read_split(f'{path}/{folder}/{folder}-{fold}.cache')\n",
    "                    \n",
    "                    ## filter data for topic\n",
    "                    df_ = df[df['topic_id'] == int(folder)]\n",
    "                    df_ = df_.dropna()\n",
    "                    df_['doc_id'] = df_.apply(map_doc, axis=1)\n",
    "\n",
    "                    df_test = df_[df_['doc_id'].isin(test)]\n",
    "                    \n",
    "                    ## genertae test data\n",
    "                    test_unbalanced, test_balanced = stratify(df_test)\n",
    "\n",
    "                    # Extract features and predict\n",
    "                    test_sentences = test_unbalanced['sentence']\n",
    "                    test_labels = test_unbalanced['label']\n",
    "                    \n",
    "                    test_extracted = [process_text_and_extract_features(sentence, label) for sentence, label in zip(test_sentences, test_labels)]\n",
    "                    X_test = [features for features, _ in test_extracted]\n",
    "                    y_test = [labels for _, labels in test_extracted]\n",
    "                    \n",
    "                    ## Predict\n",
    "                    y_pred = predict_with_crf(crf_model, X_test)\n",
    "                    results_df = evaluate_model_predictions(y_test, y_pred, fold, results_df, model_save_dir, folder)\n",
    "                    \n",
    "                else:\n",
    "                    print(f\"Model file not found: {fold_model_path}\")\n",
    "                    \n",
    "            print(\"\\n Completed all folds\")\n",
    "            # Save results to CSV\n",
    "            results_csv_path = os.path.join(model_save_dir, folder, \"final_results.csv\")\n",
    "            results_df.to_csv(results_csv_path, index=False)\n",
    "            print(f\"Results saved to {results_csv_path}\")\n",
    "            \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9288ac6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'core/qrels/'\n",
    "model_save_dir = 'raw_data_exp_25_04_24'\n",
    "folders_to_process = ['1272', '1474','1238', '1275', '1239', '1520', '1509', '1240', '1308', '1319', '1439', \n",
    "                 '1267', '1242', '1462', '1265', '1444', '1312', '1244', '1243', '1468', '1309', '1524', \n",
    "                 '1247', '1440', '1251', '1249', '1248', '1262', '1250', '1252', '1245', '1512', '1498', \n",
    "                 '1601', '1443', '1086', '1551', '1253', '1320', '1304', '1469', '1611', '1300', '1489', \n",
    "                 '1500', '1261', '1318', '1460', '1475', '1321']\n",
    "\n",
    "\n",
    "results_df = test_crf_model(model_save_dir, folders_to_process, path, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9cb75f",
   "metadata": {},
   "source": [
    "### Calculating mean across topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118ea7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = []\n",
    "# Step 1: Loop through each topic ID and read the CSV file\n",
    "for topic_id in folders_to_process:\n",
    "    csv_path = os.path.join('./',str(topic_id), \"final_results.csv\")\n",
    "    print(csv_path)\n",
    "    df = pd.read_csv(csv_path)\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Step 2: Concatenate all DataFrames\n",
    "combined_df = pd.concat(dataframes)\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be09952c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean for each topic across all folds\n",
    "topic_means = combined_df.groupby('Topic ID').mean().drop(columns='Fold')\n",
    "\n",
    "# Calculate the mean of means across all topics\n",
    "overall_means = topic_means.mean()\n",
    "\n",
    "# Print results\n",
    "print(\"Mean Metrics for Each Topic:\")\n",
    "print(topic_means)\n",
    "print(\"\\nOverall Mean Metrics Across All Topics:\")\n",
    "print(overall_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7912850",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
